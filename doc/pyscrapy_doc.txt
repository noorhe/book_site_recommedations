приоретизатор должен работать на этапе инициализации
    проблема в том, что приоретизатор хранит реквесты, а при инициализации их не создается
    вариант: хранить в приоретизаторе не реквесты, а лог-записи, по которым потом можно сформировать реквесты
так, надо сформулировать проболему
    в чем она?
    лимит загрузки исчерпывается не в связи с тем, какие айтемы выдвает приоретизатор
    т.е. приоретизатор то, конечно, выбирается айтемы, но только из тех, что смогли залететь в него, не превысив лимит
    получается, что более приоритетные айтемы могут быть попросту не выбраны, так как лимит уже был исчерапн менее приоритетными
    значит, лимит должен изменяться после того, как айтем был выбран из приоретизатора?
        к приоретизатору обращение идёт в двух случаях:
            сразу после инициализации
            после успешного парсинга очередного реквеста, чтобы сделать следующий реквест
        нужно ли сначала проверять
    приоретизатор сначала должен выбрать

так, приоретизатор может быть
а что с инциализацией?
я думаю, нужны приоретизатор и дедупликтор
урлы, прошедшие первый, попадают во второй
после того, как урла загружена, она снова попадает в дедупликтор
нужна приоретизация списка парсинга, чтобы скорее получить законченные рид листы
    что должен делать такой приоретизатор
        какие вообще есть реквесты?
        user_id_page_pair
        book_url
        reader_list_url
        book_tag_pair
        book_selections_url
        rating_distributions_quartet
    он должен давать приоретет более высоким номерам страницы в user_id_page_pair
    если у него есть ссылки на:
        book_url
        book_tag_pair
        book_selections_url
        rating_distributions_quartet
    он должен дать приоретет им
    должны ли reasers_list ссылки меть самый выскоий приоритет?
        если да, то должны ли их версии с более выскоими page_num иметь более высокий приоритет?
    что если да? тогда как будет выглядеть алгоритм
    дискретные приоритеты
        user_id_page_pair 1 (при этом, чем выше номер страницы, тем выше приоритет)
        book_url 2
        book_tag_pair 2
        book_selections_url 2
        rating_distributions_quartet 2
        reader_list_url 3 (чем выше номер страницы, тем выше приоритет)
    интерфей приоретизатора
        у приоретизатора есть 6 методов, для каждого типа данных, которые может выдавать в последвие для созщдания реквеста
        есть 1 метод, взять следующий кусок данных для создания реквеста
    общий алгоритм работы
        *каждая коллекция для урлов, в которых есть номер страницы является очередью в приоритетом по странице, в которой страницы в более высокими номерами имеют приоритет
        каждый метод, который помещает тип данных в приореизатор, работает со своей коллекцией
        при вставке он просто вставляется в этой коллекцию
            если тип данных с номером страницы, вставляет эту страницы как приоритет для очереди с приоритетом
        при выемке следующей куска данных
            приоретизатор пробегается по своим коллекциям начиная с тех, у которых значение приоритета меньше к тем, у которых больше
            в случае, если значение приоритета у N колекций одинаковое, вычисляется равномерно распределнное случайное число от 0 до N-1, и из коллекции, соответствующей числу берется след чанк данных




что если просто копировать список для парсинга, и уже по копии пробегаться?
    если обьект в одном ищ сетов, мы его не парсим
    допустим мы скопировали сет, что будет дальше
        мы начинаем пробегаться по копии
        копия остается неизменной
        из оригинального сета мы достаем обьекты и перекладываем во второй
        может ли быть такое, что обьект уже распарсили, он уже не в оригинальном сете, но все еще в копии? и мы снова его распарсим?

так, какие еще доделки нужны
	принимать из команд лайна аргумента по максимальным загрузкам обтектов
	скидывать в файл статы загрузки

теперь нужен раннер
раннер должен
	запуститься по времени
	отменить дальнейшие запуски раннера
	сгенерить лимиты кроулера
	запустить кроулер
	дождаться, когда кроулер отработает
	включить дальнейшие запуски раннера
вообще, идея раннера в том, что его можно запустить один раз из командной строки, он сам пропишет себя в крон
	значит, при втором ране он не должен прописаться в крон снова
		как этого добиться?
			на самом деле он должен получить джобу из крона
			если её там нет, создать её там и задисейблить
			сгенерить лимиты кроулера
			запустить кроулер
			дождаться, когда кроулер отработает
			включить дальнейшие запуски раннера

тестовый прогон
 ошибки
 	404
 	traceback
 	нет останова по истечении лимита

для каждого tr
  певый td - величина оценками
  третий td - процент поставивших оценку
  четвертый td - количество поставивших оценку
предпоследний td - не поставили оценку - можно не учитывать, я думаю

Год издания: 2009

Надо как-то загрузить подпоборки, в которые входит книга.
    ЧТобы это сделать, надо или
        запустить джава скрипт от имени определенного элемента на странице
        или как-то найти способ загрузить подобрки по-другому
            https://selenium-python.com/install-geckodriver
                driver = webdriver.Firefox('/путь/до/драйвера/geckodriver')

Дополнительные параметры книги
    Средняя оценка
    Количество читателей
    Детали оценки
        5: количество
        4: количество
        3: количество
        2: количество
        1: количество
    Количество планирующих прочитать
    Издательство
    Год
    Кол-во страниц
    Серия
    Цикл
    Номер в цикле
    Язык
    Тип обложки
    Возрастные ограничения
        Подборки



Нужна возможность продолжить кровлинг с того места, где остановились
В идеале сделать унифицированную систему холодного (с самого начала) и горячего (продолжить с того места, где остановились) старта
    Для этого нужно
        множество урлов для парсинга
        множество урлов, которые уже были распаршены
        метод, который загрузит урлы из дампов
    Думаю, одним из вариантов было бы сделать не дамп данных, а лог, в которм будут push и pull операции, и восстанавливать дамп из такого лога
    Когда в логе запись на пуш, нужно
        положить

Проблема юникод символов в том, как формируется line в ItemAdapter

Scrapy useful settings
    https://docs.scrapy.org/en/latest/topics/settings.html#built-in-settings-reference
        RANDOMIZE_DOWNLOAD_DELAY


Котносительным урлам надо везде, где генерится Реквест, прибавить слева имя домена
Там надо вычленить из ссылок серединки

Может быть слегка поменять концепцию. Сделать айтемом не рид лист, а отдельную книги, а её оценки - это просто словарь вида: айдиПользователя
Что если немного нормализовать данные. Сделать айтемы плоскими, без вложенных сущностей.
Т.е. одни айтемы будут вида
    userId
    bookId
    rate
А другие будут - плоские данные книги

Тогда алгоритм будет выглядеть так
    когда пасрер стартует
        генератор реквестов по юзерАйди(userIds: List[{userId: String, pageNum: Integer}])
            Взять список следующий userId из userIds
            Сформировать из них реквесты
        выкинуть реквесты в движок
        процесс переходит в parse
    в методе parse
        выкинуть как айтемы тройки
            userId
            bookId
            rate
        добавить в сет книг на парсинг книги с текущей страницы
            сгенерировать реквесты книг для парсинга
        выкинуть реквесты книг для парсинг в движок
        процесс переходит в parseBook callback
        убрать из списка userReadListPages распарсенную страницу
        добавить в список userReadListPages слдующую страницу
        вызвать генератор реквестов по userId (после их завершения процесс вёрнет респонсы сюда, в метод parse)
    в методе parseBook
        бросаем как айтем
            id книги
            название книги
            автор
        для каждого жанра
            бросаем как айтем
                id книги
                жанр
        для каждого тэга
            бросаем как item
                id книги
                тэг
        добавляем в сет пользователей для парсинга читателей книги
        генереуем реквесты по читателям
        переходим в метод parse

Проблема:
    уточняя тэги книги по искусственному id в рид листе, мы будем по многу раз уточнять тэги одних и тех же книг
    решение: уточнение данных книги по урл-у книги. это должно быть достаточно уникально, по крайней мере в течение несколких последовательных ранов сканнера

Так, сначала парсим страницу рид листа
Потом генерируем запросы на уточнение тэгов
Потом генерируем запросы на пользователей
Потом генерируем запрос на следующую страницу
    Если слудующей страницы нет, выбрасываем рид лист как айтем


type Context
    userIdToReadListDict: Dict<userId: string -> readList: ReadList)>
    userToParseIds: Set<String>
type ReadList
    bookIdToBookDataDict: Dict<artificaialBookId: string -> BookData>

Можно попробовать хранить все в глобальных переменных внутри спайдера
Или можно использовать обьект, типа Context
    type Context
        read_listst
В какой момент бросать очередной рид лист как айтем в пайплан?
    Т.к. сначала бросаются и обраюатываются все другие реквесты, я думаю, бросать айтем можно тогда, когда главная функция (parse) прошло дошла до конца

Реквесты по пользователям -> parse
    Значит, по пользователю, получаем с каждым реквестом страницу результатов - список книг в рид листе пользователя с оценками этого пользователя
        Для некоторых книг надо сформировать реквесты на зайти в книгу и скачать тэги
        После обработки страницы рид листа надо сформировать и вернуть реквест для перехода на следующую страницу
Реквесты по тэгам -> parse_tags
Реквесты по читателям -> parse_readers

Берем список реквестов по пользователям
Обрабатываем в коллбэке parse
На каждый респонс йелдим
    Список чтения
        Что сформировать список чтения
    Новые реквесты для новых пользователей

Вообще, что является единицей сохраняемых данных?
    Список чтения пользователя

Что если сначала загрузить все данные?
    Все данные одного списка чтения или вообще все?
    Допустим, одного списка чтения


Общий алгоритм
    взять id-шник данного пользователя
    пока есть следующая страница в списке прочитанных данного пользователя
        получить следующую страницу прочитанных книг для данного пользователя
        распарсить прочитанные со страницы
    для каждой книги
        если нужно
            зайти в книгу и распарсить тэги
        зайти в книгу и добавить всех прочитавших в список пользователей для парсинга

Что если завести обьект context, который передавать в коллбэки?
    Что он будет содержать?
        Список прочитанных (результаты?)
        Текущего пользователя
        Текущую книгу
        Текущую страницу
ОК, предположим мы передаём его в колл бэк для загрущки дополнительных данных

Какие типы задач существуют
    парсинг страницы списка чтение конкретного пользователя


Я так понял, что возможно параллельное выполнение, если parse готов возвращать и возвращать реквесты

Так.. как мне дополнить НЕКОТОРЫЕ из предыдущих проскраленных списков чтения?
    нужно сначала сделать запрос, чтобы зайти внутрь книги
    скачать тэги
    дополнить тэгами айтем с книгой

По сути, callback обрабатывает результаты только данного конректного реквеста.
А что если, эти результаты должны обрабатываться на основе результатов других реквестов?
Возможные решения:
    1. item pipelines
    2. замыкать контекст, нужный для отработки коллбэка
    3. глобальный переменные
    4. ?
